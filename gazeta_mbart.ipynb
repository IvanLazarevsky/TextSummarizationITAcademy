{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерируем аннотации при помощи модели mBart. Замеряем ROUGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartTokenizer, MBartForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"IlyaGusev/mbart_ru_sum_gazeta\"\n",
    "tokenizer = MBartTokenizer.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 28 16:33:38 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3090    Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   55C    P2   137W / 350W |   5318MiB / 24265MiB |     60%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1110      G   /usr/lib/xorg/Xorg                 84MiB |\r\n",
      "|    0   N/A  N/A      1657      G   /usr/lib/xorg/Xorg                194MiB |\r\n",
      "|    0   N/A  N/A      1788      G   /usr/bin/gnome-shell              153MiB |\r\n",
      "|    0   N/A  N/A      2209      G   ...AAAAAAAAA= --shared-files       63MiB |\r\n",
      "|    0   N/A  N/A      3729      C   ...thon/general37/bin/python     4803MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MBartForConditionalGeneration(\n",
      "  (model): MBartModel(\n",
      "    (shared): Embedding(250027, 1024, padding_idx=1)\n",
      "    (encoder): MBartEncoder(\n",
      "      (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): MBartEncoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): MBartDecoder(\n",
      "      (embed_tokens): Embedding(250027, 1024, padding_idx=1)\n",
      "      (embed_positions): MBartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (10): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (11): MBartDecoderLayer(\n",
      "          (self_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): MBartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=250027, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAZETA_PATH = '../data/gazeta_jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazeta_records(file_name, shuffle=False, sort_by_date=True):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'train': os.path.join(GAZETA_PATH,'gazeta_train.jsonl'),\n",
    "    'val': os.path.join(GAZETA_PATH,'gazeta_val.jsonl'),\n",
    "    'test': os.path.join(GAZETA_PATH, 'gazeta_test.jsonl')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {\n",
    "    split: read_gazeta_records(path) for split, path in dataset_files.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = records['val'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Будущее капитана московского «Спартака» Дениса Глушакова весь этот сезон находится в подвешенном состоянии, и, похоже, развязка уже близка. Красно-белых ждет серьезная перестройка в летнее трансферное окно, и новому генеральному директору команды Томасу Цорну поставили задачу максимально выгодно расстаться с некоторыми футболистами — в том числе и с 32-летним полузащитником, который ощутимо сдал по игровым кондициям и впал в немилость у большинства фанатов клуба из-за скандальной ссоры с Массимо Каррерой. Однако по контракту у Глушакова еще остается год игры за «Спартак» с зарплатой 3 млн евро в год, как сообщает «СЭ». В случае досрочного расторжения клуб должен будет выплатить футболисту половину этой суммы. И вот тут начинается самое интересное. Адвокат бывшей жены Глушакова Дарьи — Сергей Жорин — заявил, что ему кажется «очень похожей на правду» информация о том, что футболист якобы попросил клуб отдать ему 1,5 млн наличными, чтобы избежать выплаты алиментов. Напомним, суд обязал футболиста выплачивать 1/3 всех доходов на содержание двух дочерей, которые остались с матерью. Необходимая сумма автоматически списывалась с карточки игрока при выплате ему зарплаты и переводилась на счет жены. Но если бы клуб согласился отдать 1,5 млн наличкой, «по-черному», то эти деньги не были бы отражены в документах клуба и налоговой декларации, и, соответственно, из них не извлекалась бы сумма на алименты. Как и налоги. «Подтвердить или опровергнуть информацию я не могу, потому что не обладаю достоверными доказательствами, но это очень похоже на правду. Как мне кажется, это в стиле Дениса Глушакова. Я надеюсь, «Спартак» не пойдет на эти уловки и ухищрения, потому что до этого момента клуб вел себя достойно. Выплачивает алименты и исполняет судебные решения», — приводит слова Жорина Sport24. Также адвокат заверил, что в его распоряжении имеется копия контракта Глушакова, так что, если футболист на бумаге вдруг получит меньше денег, чем ему полагается по договору, и не будет возражать, то это станет поводом для пристального изучения. «В таком случае мы будем запрашивать информацию в судебном порядке и ее проверять. Если она подтвердится, то это может быть ряд уголовных составов, начиная от уклонения уплаты алиментов, умышленного неисполнения решения суда, и вплоть до мошенничества», — цитирует адвоката «СЭ». При этом Жорин отметил, что его клиентка Дарья уже не удивляется подобным новостям о возможных планах своего бывшего мужа, поскольку очень хорошо знает его как личность. «Я думаю, что с Денисом в «Спартаке» расторгают контракт «благодаря» его хитростям и поведению, а в завершение идти на поводу Глушакова — это неразумно, на мой взгляд», — подытожил юрист. После появления этих слухов и комментариев на авансцену вышла и адвокат самого Глушакова — Марина Дубровская. Девушка заверила, что ее подопечный вовсе не собирался пользоваться подобной схемой. «Мы уже привыкли к сливам неправдивой информации со стороны оппонентов на одних и тех же ресурсах, но здесь они превзошли сами себя. Конечно же, вся информация о финансовых требованиях Дениса к «Спартаку» — полнейшая ложь. И в клубе вам могут это подтвердить», — заявила юристка. Напомним, настолько кардинальные расхождения во взглядах у противоборствующих сторон происходили и ранее. Так, адвокат жены Глушакова Жорин в начале мая заявлял, что футболист якобы написал заявление в Федеральную службу безопасности ( ФСБ ) и запретил выпускать его общего с Дарьей ребенка за границу. «Глушаков решил отомстить за то, что суд оставил детей с мамой и взыскал с него алименты. Накануне праздников он написал заявление в ФСБ с требованием не выпускать старшую дочь за пределы России. Ребенок узнал о том, что никуда не едет, на паспортном контроле. Молодец, Денис!» — написал Жорин в своем твиттере. Закон действительно предусматривает возможность написать подобное заявление в пограничную службу ФСБ, чтобы предупредить такие случаи, когда один из родителей пытается тайком увезти ребенка (особенно если у него есть гражданство другой страны). «По моему мнению, Глушаков просто злоупотребил своим правом, а федеральная пограничная служба не обязана проверять обоснованность мер, поскольку достаточно лишь написать заявление», — высказался Жорин в комментарии «Чемпионат.com». Адвокат Глушакова Дубровская тогда огласила свою версию происходящего. По словам юристки, Дарья «со своим бойфрендом» пыталась вывезти детей в Дубай еще в самом начале бракоразводного процесса. Естественно, это насторожило футболиста, который не хотел лишаться возможности видеть своих дочерей. «Тогда Денис написал заявление в аэропорт и установил запрет на выезд детей. Дарья об этом запрете прекрасно знала на протяжении полугода. Зачем она повезла дочь в аэропорт, чтобы травмировать ее психику, нам совершенно не понятно», — заявила Дубровская ТАСС , прибавив, что после отказа дочери на выезд Дарья спокойно оставила ребенка в России и улетела отдыхать. Кто в этой ситуации прав, а кто виноват, установить решительно невозможно. А пока продолжается трудный бракоразводный процесс, в ходе которого Глушаков пытается снизить размер алиментов. Капитан «Спартака» настаивает на том, чтобы перечислять фиксированную сумму на содержание бывшей семьи — 300 тысяч рублей в месяц. Также адвокаты футболиста обнаружили ошибки в протоколе заседания в Никулинском суде, который определил размер алиментов. По этой причине дело было вновь возвращено в районный суд. «Сторона Глушакова подала замечание на протокол и сопроводила это заявлением о восстановлении срока. Поскольку срок на подачу этих замечаний был пропущен по объективным обстоятельствам, было подано это заявление. Но подано оно было после апелляционной жалобы, поэтому Московский городской суд возвращает дело в Никулинский суд. После рассмотрения там его опять вернут в Мосгорсуд. Дело затягивается», — цитировал Sport24 одного из адвокатов Дарьи Алису Образцову. Другие новости и материалы можно посмотреть на странице хроники , а также в группах отдела спорта в социальных сетях Facebook и «Вконтакте» .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/Programming/Python/general37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3226: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.prepare_seq2seq_batch(\n",
    "    [article_text],\n",
    "    src_lang=\"en_XX\", # fairseq training artifact\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=600\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        input_ids=input_ids.cuda(),\n",
    "        max_length=162,\n",
    "        no_repeat_ngram_size=3,\n",
    "        num_beams=10,\n",
    "        top_k=0\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Защитник московского «Спартака» Денис Глушаков заявил, что ему кажется «очень похожей на правду» информация о том, что футболист якобы попросил клуб отдать ему 1,5 млн наличными, чтобы избежать выплаты алиментов.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(refs, hyps, metric=\"all\"):\n",
    "    metrics = dict()\n",
    "    metrics[\"count\"] = len(hyps)\n",
    "    metrics[\"ref_example\"] = refs[-1]\n",
    "    metrics[\"hyp_example\"] = hyps[-1]\n",
    "\n",
    "    if metric in (\"rouge\", \"all\"):\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "        metrics.update(scores)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(refs, hyps, metric=\"all\"):\n",
    "    metrics = calc_metrics(refs, hyps, metric=metric)\n",
    "\n",
    "    print(\"-------------METRICS-------------\")\n",
    "    print(\"Count:\\t\", metrics[\"count\"])\n",
    "    print(\"Ref:\\t\", metrics[\"ref_example\"])\n",
    "    print(\"Hyp:\\t\", metrics[\"hyp_example\"])\n",
    "\n",
    "#     if \"bleu\" in metrics:\n",
    "#         print(\"BLEU:     \\t{:3.1f}\".format(metrics[\"bleu\"] * 100.0))\n",
    "    if \"rouge-1\" in metrics:\n",
    "#         print([metrics[\"rouge-1\"][m] * 100.0 for m in ('p','r','f')])\n",
    "        print(\"ROUGE-1: P: {:3.2f} R: {:3.2f} F: {:3.2f}\".format(\n",
    "            *[metrics[\"rouge-1\"][m] * 100.0 for m in ['p','r','f']]))\n",
    "        print(\"ROUGE-2: P: {:3.2f} R: {:3.2f} F: {:3.2f}\".format(\n",
    "            *[metrics[\"rouge-2\"][m] * 100.0 for m in ['p','r','f']]))\n",
    "        print(\"ROUGE-L: P: {:3.2f} R: {:3.2f} F: {:3.2f}\".format(\n",
    "            *[metrics[\"rouge-l\"][m] * 100.0 for m in ['p','r','f']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(refs, hyps, tokenize_after=True, lower=True):\n",
    "    for i, (ref, hyp) in enumerate(zip(refs, hyps)):\n",
    "        ref = ref.strip()\n",
    "        hyp = hyp.strip()\n",
    "        if tokenize_after:\n",
    "            hyp = \" \".join([token.text for token in razdel.tokenize(hyp)])\n",
    "            ref = \" \".join([token.text for token in razdel.tokenize(ref)])\n",
    "        if lower:\n",
    "            hyp = hyp.lower()\n",
    "            ref = ref.lower()\n",
    "        refs[i] = ref\n",
    "        hyps[i] = hyp\n",
    "    return refs, hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_device(model):\n",
    "    return next(iter(model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_method_score(records, predict_func, nrows=None, return_ref_pred=False, text_key='text'):\n",
    "    references = []\n",
    "    predictions = []\n",
    "\n",
    "    for i, record in tqdm(enumerate(records)):\n",
    "        if nrows is not None and i >= nrows:\n",
    "            break\n",
    "        summary = record[\"summary\"]\n",
    "        text = record[text_key]\n",
    "        prediction = predict_func(text, summary)\n",
    "        references.append(summary)\n",
    "        predictions.append(prediction)\n",
    "    references, predictions = postprocess(references, predictions)\n",
    "    print_metrics(references, predictions)\n",
    "    if return_ref_pred:\n",
    "        return references, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция предсказания аннотаций. Снизьте num_beams для ускорения (может понизить качество). В качестве refsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_bart(text):\n",
    "    input_ids = tokenizer.prepare_seq2seq_batch(\n",
    "        [text],\n",
    "        src_lang=\"en_XX\", # fairseq training artifact\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=600\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids.cuda(),\n",
    "            max_length=162,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=10,\n",
    "            top_k=0\n",
    "        )[0]\n",
    "        \n",
    "        \n",
    "    summary = tokenizer.decode(output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сенаторы Соединенных Штатов предлагают ввести пять видов санкционных ограничений против тех, кто страхует суда, укладывающие «Северный поток — 2». Об этом сообщается на сайте конгресса. «Президент может ввести пять или более санкций… в отношении иностранного лица, если он решит, что это лицо осознанно в дату вступления этого закона в силу или после нее предоставит услуги по оценке рисков, страхованию или перестрахованию судну», — указывается в законопроекте. Законопроект предусматривает, что ограничения в первую очередь будут вводиться против иностранных компаний, которые предоставляют свои специализированные суда для укладки газовых труб в Балтийском море. Суда для строительства Nord Stream 2 предоставляют три компании: швейцарская Allseas, итальянская Saipem и российская МРТС («Межрегионтрубопроводстрой»). Помимо этого санкции должны быть введены и против юридических или физических лиц, которые сознательно оказывают страховые или гарантийные услуги этим судам. Законопроект обяжет собрать данные обо всех участниках строительства СП — 2. И если участие в проекте подтверждается, то США могут отказать таким лицам в выдаче или продлении кредита на общую сумму более $10 млн в течение года. Может также последовать отказ в выдаче лицензии или разрешения на экспорт любых товаров или технологий регуляторами США. Кроме того, перед работающими над газопроводом компаниями маячит запрет на работу с госдолгом США или федеральными средствами «в качестве основного дилера». Участие в СП-2 грозит запретом на проведение платежей в зоне американской юрисдикции и сделок с собственностью, запретом инвестировать в капитал. Также правительство Соединенных Штатов, говорится в документе, не может заключать какие-либо контракты на закупку товаров или услуг у замешанного в проекте иностранного лица. Директорам и акционерам с контрольным пакетом акций в таких компаниях могут отказать в выдаче американской визы и запретить въезд в США. Отмечается, что авторами законопроекта являются Джин Шахин, Тед Круз , Том Коттон и Джон Баррассо. Черновой вариант законопроекта был подготовлен представителями обеих парламентских партий еще 14 мая, что повышает шансы на принятие документа. Джин Шахин — демократ. А Тед Круз, известный своим негативным отношением к России, является республиканцем и экс-кандидатом в президенты США. Как ранее сообщало агентство Bloomberg, глава Минэнерго США Рик Перри заявил, что Вашингтон планирует ввести санкции против «Северного потока — 2». О таких планах Перри заявил 21 мая во время выступления в Киеве. «Палата представителей и сенат, как ожидается, подготовят законопроект об ограничениях для компаний, участвующих в проекте Nord Stream 2», — пригрозил министр. Вместо русского газа чиновники из США продвигают в Европу свой СПГ. И даже наполнили американский сжиженный газ метафорическим смыслом. Свое топливо они называют «молекулы американской свободы». На вопрос: «Чем пахнет свобода?» министерство энергетики США отвечает честно и прямо: газом. Выступая в Ванкувере на министерской встрече по чистой энергии, заместитель министра энергетики Марк Менезес заявил, что увеличение экспорта американского сжиженного природного газа имеет «решающее значение для распространения свободного газа по всему миру», ведь так тогда везде будет «более чистый воздух». Поставки американского газа нужно увеличить для «обеспечения мира чистой энергией, а также для поддержки энергетической безопасности союзников США», поддержал его помощник министра энергетики по ископаемым видам топлива Стивен Винберг. «Это позволяет молекулам американской свободы распространяться по всему миру», — подчеркнул Винберг. Позже ведомство лиричные цитаты обоих опубликовало на своем сайте. США достаточно давно закрепили за собой монополию на «экспорт» свободы и демократии во всем мире, поэтому, очевидно, газ, добываемый в самом демократическом, по мнению Вашингтона, государстве, и называется свободным, отмечает директор Фонда развития права и медиации ТЭК Александр Пахомов. «Если отбросить весь пафос, то мы обнаружим, что за ним скрывается вполне реальная перспектива нового витка экспансии американского СПГ на мировой рынок», — констатирует эксперт. С июля 2018 года поставки американского СПГ в Евросоюз увеличились на 272% — до 10,4 миллиарда кубометров. Минувшей зимой США заняли третье место по объему поставок в ЕС с долей 12,6%. В первом квартале этого года поставки СПГ из США на экспорт составили 10,2 млрд куб. м, в то время как в первом квартале прошлого года 6,6 млрд куб. м — рост на 55%. Сейчас в США работают три экспортных завода СПГ, в то время как в первом квартале прошлого года — только один. Еще три завода сейчас находятся на стадии ввода в эксплуатацию, а два — в стадии строительства. Мощность уже работающих СПГ производств составляет свыше 40 млрд куб. м газа, а вместе со сдающимися в эксплуатацию и строящимися вырастет к 2025-му до свыше 130 млрд куб. м газа, отмечает аналитик по газу Центра энергетики Московской школы управления Сколково Сергей Капитонов. Правда, методы продвижения американского СПГ мало ассоциируются со словом «свобода». На европейском рынке традиционно сильны позиции России, которая десятилетиями поставляет трубный газ в страны ЕС. Американские компании свою экспансию начали не так давно, однако это движение активно подкрепляется на уровне Белого дома. Президент Дональд Трамп неоднократно заявлял о том, что для своей энергобезопасности ЕС стоит отказываться от российских энергоносителей в пользу поставок со стороны союзников, то есть США.\n"
     ]
    }
   ],
   "source": [
    "print(records['val'][1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ни много ни мало, а пять видов санкций предлагают ввести США против участвующих в российском проекте «Северный поток — 2». Замешанным в СП — 2 запретят любые финансовые операции, в том числе выдачу кредитов. Взамен российского газа США продвигают «молекулы свободы» — свой сжиженный природный газ.\n"
     ]
    }
   ],
   "source": [
    "print(records['val'][1]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/Programming/Python/general37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:3226: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Глава Минэнерго США Рик Перри заявил, что Вашингтон планирует ввести санкции против компаний, участвующих в строительстве газопровода «Северный поток — 2». Законопроект предусматривает пять видов санкционных ограничений против тех, кто страхует суда, укладывающие газовые трубы в Балтийском море. Авторами законопроекта являются Джин Шахин, Тед Круз, Том Коттон и Джон Баррассо. Черновой вариант законопроекта был подготовлен представителями обеих парламентских партий еще 14 мая, что повышает шансы на принятие документа.\n"
     ]
    }
   ],
   "source": [
    "print(predict_with_bart(records['val'][1]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(4543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b809efa61a6447c396cdaf592174a30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "|          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------METRICS-------------\n",
      "Count:\t 70\n",
      "Ref:\t украинская певица светлана лобода пожаловалась подписчикам на жуткие гематомы . согласно артистке , травмы стали следствием ее концертной деятельности .\n",
      "Hyp:\t украинская певица светлана лобода поделилась с подписчиками фотографией своих ног , на которых отчетливо видны огромные гематомы . по словам 36-летней уроженки киева , данные травмы стали прямым следствием ее активной концертной деятельности . ранее в сми появилась информация , что лидер немецкой метал-группы rammstein тилль линдеманн сломал челюсть поклоннику из-за девушки .\n",
      "ROUGE-1: P: 33.43 R: 38.19 F: 34.50\n",
      "ROUGE-2: P: 14.35 R: 16.64 F: 14.87\n",
      "ROUGE-L: P: 29.62 R: 33.25 F: 30.46\n"
     ]
    }
   ],
   "source": [
    "refs, preds = calc_method_score(random.sample(records['val'],70), \n",
    "                                lambda x,y: predict_with_bart(x), \n",
    "                                return_ref_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "верховный суд ес снял санкции с экс-президента украины виктора януковича и шести его приближенных . судьи решили , что замораживание их счетов и активов было проведено в 2016-2018 годах незаконно . ограничительные меры были сняты по причине отсутствия достаточных доказательств того , что чиновники выводили похищенные государственные средства за рубеж .\n"
     ]
    }
   ],
   "source": [
    "print(refs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "европейский суд отменил санкции в отношении бывшего президента украины виктора януковича и его окружения , отменив тем самым решение европейского совета по продлению санкций в их отношении . по мнению судей , замораживание счетов и активов названных лиц в 2016-2018 годах было незаконным .\n"
     ]
    }
   ],
   "source": [
    "print(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bart_result.txt', 'w+') as f:\n",
    "    for ref, hyp in zip(refs, preds):\n",
    "        f.write(ref)\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(hyp)\n",
    "        f.write(\"\\n\\n=============\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
