{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAZETA_PATH = 'data/gazeta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazeta_records(file_name, shuffle=False, sort_by_date=True):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\", encoding='utf-8') as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'train': os.path.join(GAZETA_PATH,'gazeta_train.jsonl'),\n",
    "    'val': os.path.join(GAZETA_PATH,'gazeta_val.jsonl'),\n",
    "    'test': os.path.join(GAZETA_PATH, 'gazeta_test.jsonl')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {\n",
    "    split: read_gazeta_records(path) for split, path in dataset_files.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_summarizer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gpt_summarizer_dataset' from 'C:\\\\Users\\\\ivan\\\\Programming\\\\DeepLearning\\\\TextSummarizationITAcademy\\\\gpt_summarizer_dataset.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(gpt_summarizer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_summarizer_dataset import GPTHeadlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gpt_training_dataset.pkl', 'rb') as f:\n",
    "#     train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gpt_val_dataset.pkl', 'rb') as f:\n",
    "#     val_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52400it [04:02, 216.17it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = GPTHeadlineDataset(\n",
    "    tokenizer,\n",
    "    summaries=[r['summary'] for r in records['train']],\n",
    "    contents=[r['text'] for r in records['train']],\n",
    "    max_input_length=601,\n",
    "    max_summary_length=163\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5265it [00:19, 269.67it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = GPTHeadlineDataset(\n",
    "    tokenizer,\n",
    "    summaries=[r['summary'] for r in records['val']],\n",
    "    contents=[r['text'] for r in records['val']],\n",
    "    max_input_length=601,\n",
    "    max_summary_length=163\n",
    ")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хак для быстрой загрузки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('gpt_training_dataset.pkl', 'wb') as of:\n",
    "    pickle.dump(train_dataset, of)\n",
    "    \n",
    "with open('gpt_val_dataset.pkl', 'wb') as of:\n",
    "    pickle.dump(val_dataset, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52400.000000\n",
       "mean        63.571126\n",
       "std         16.982161\n",
       "min         17.000000\n",
       "25%         51.000000\n",
       "50%         63.000000\n",
       "75%         75.000000\n",
       "max        123.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataset.summary_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52400.000000\n",
       "mean       954.682214\n",
       "std        273.714437\n",
       "min         48.000000\n",
       "25%        780.000000\n",
       "50%        901.000000\n",
       "75%       1088.000000\n",
       "max       2244.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataset.content_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(learning_rate=6e-5, \n",
    "                    warmup_steps=1000,\n",
    "                    linear_decay_steps=0,\n",
    "                    content_loss_weight=1,\n",
    "                    summary_loss_weight=1,\n",
    "                    pretrained_model_path=model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPTSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GPTSummarizer' from 'C:\\\\Users\\\\ivan\\\\Programming\\\\DeepLearning\\\\TextSummarizationITAcademy\\\\GPTSummarizer.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(GPTSummarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPTSummarizer import GPTSummarizerPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTSummarizerPL(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_device(model):\n",
    "    return next(iter(model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_device(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(model, text, max_input_length, max_output_length, **generate_args):\n",
    "    vocab=tokenizer.get_vocab()\n",
    "    bos_token_id = vocab['<s>']\n",
    "    eos_token_id = vocab['</s>']\n",
    "    pad_token_id = vocab['<pad>']\n",
    "    encoded_text = [bos_token_id] +\\\n",
    "        tokenizer.encode(text)[:max_input_length] + [eos_token_id]\n",
    "    encoded_text = torch.tensor(encoded_text, device=get_model_device(model)).view(1,-1)\n",
    "    print(encoded_text.shape)\n",
    "    encoded_output = model.gpt.generate(encoded_text,\n",
    "                                        bos_token_id=bos_token_id,\n",
    "                                        eos_token_ids=[eos_token_id],\n",
    "                                        pad_token_id=pad_token_id,\n",
    "                                        max_length=max_input_length + max_output_length + 2,\n",
    "                                        **generate_args)\n",
    "    result = tokenizer.decode(encoded_output[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "n_workers=0\n",
    "data_loaders = {\n",
    "    \"train\": DataLoader(train_dataset, \n",
    "                        batch_size=batch_size, num_workers=n_workers,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=train_dataset.collate),\n",
    "    \"val\": DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        num_workers=n_workers,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=val_dataset.collate),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingCallback(Callback):\n",
    "    def on_validation_end(self, trainer, module):\n",
    "        state = module.training\n",
    "        module.train(False)\n",
    "        \n",
    "        rand_index = random.randrange(len(records['val']))\n",
    "        text = records['val'][rand_index]['text']\n",
    "        with torch.no_grad():\n",
    "            print(generate_headline(module, text, 600, 100))\n",
    "        module.train(state)\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if trainer.global_step % 1000 == 0:\n",
    "            state = pl_module.training\n",
    "            pl_module.train(False)\n",
    "\n",
    "            rand_index = random.randrange(len(records['val']))\n",
    "            text = records['val'][rand_index]['text']\n",
    "            with torch.no_grad():\n",
    "                print(generate_headline(pl_module, text, 600, 100))\n",
    "            pl_module.train(state)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='avg_val_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=1,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('gpt_checkpoint_gazeta', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"gpt_checkpoint_gazeta\",monitor='avg_val_loss', mode='min', save_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[0],max_epochs=5, accumulate_grad_batches=8,\n",
    "                     callbacks=[checkpoint, early_stop_callback, SamplingCallback()], fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader=data_loaders['train'], val_dataloaders=data_loaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({'model': model.gpt.state_dict, 'hparams': model.hparams}, 'gpt_ckpt_after_epoch_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_device(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,4,5].index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(model, text, max_input_length, max_output_length, **generate_args):\n",
    "    vocab=tokenizer.get_vocab()\n",
    "    bos_token_id = vocab['<s>']\n",
    "    eos_token_id = vocab['</s>']\n",
    "    pad_token_id = vocab['<pad>']\n",
    "    encoded_text = [bos_token_id] +\\\n",
    "        tokenizer.encode(text)[:max_input_length] + [eos_token_id]\n",
    "    encoded_text = torch.tensor(encoded_text, device=get_model_device(model)).view(1,-1)\n",
    "    print(encoded_text.shape)\n",
    "    encoded_output = model.gpt.generate(encoded_text,\n",
    "                                        bos_token_id=bos_token_id,\n",
    "                                        eos_token_ids=[eos_token_id],\n",
    "                                        pad_token_id=pad_token_id,\n",
    "                                        max_length=max_input_length + max_output_length + 2,\n",
    "                                        **generate_args)\n",
    "    \n",
    "    indices = encoded_output[0].tolist()\n",
    "    \n",
    "    first_eos_index = indices.index(eos_token_id)\n",
    "    sum_start_index = first_eos_index + 1\n",
    "    \n",
    "    final_indices = []\n",
    "    for idx in indices[sum_start_index:]:\n",
    "        if idx != eos_token_id:\n",
    "            final_indices.append(idx)\n",
    "        else:\n",
    "            break\n",
    "    return tokenizer.decode(final_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_summary(model, text, max_input_length, max_output_length, **generate_args)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    rand_index = random.randrange(len(records['val']))\n",
    "    print(rand_index)\n",
    "    text = records['val'][rand_index]['text']\n",
    "    print(text)\n",
    "    print(\"-----------\")\n",
    "    ref = records['val'][rand_index]['summary']\n",
    "    print(ref)\n",
    "    print(extract_summary(model,text, 600,128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
