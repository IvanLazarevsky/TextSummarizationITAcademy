{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAZETA_PATH = '../data/gazeta_jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gazeta_records(file_name, shuffle=False, sort_by_date=True):\n",
    "    assert shuffle != sort_by_date\n",
    "    records = []\n",
    "    with open(file_name, \"r\") as r:\n",
    "        for line in r:\n",
    "            records.append(json.loads(line))\n",
    "    if sort_by_date:\n",
    "        records.sort(key=lambda x: x[\"date\"])\n",
    "    if shuffle:\n",
    "        random.shuffle(records)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_files = {\n",
    "    'train': os.path.join(GAZETA_PATH,'gazeta_train.jsonl'),\n",
    "    'val': os.path.join(GAZETA_PATH,'gazeta_val.jsonl'),\n",
    "    'test': os.path.join(GAZETA_PATH, 'gazeta_test.jsonl')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = {\n",
    "    split: read_gazeta_records(path) for split, path in dataset_files.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"sberbank-ai/rugpt3medium_based_on_gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)\n",
    "# model = GPT2LMHeadModel.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_summarizer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'gpt_summarizer_dataset' from '/home/ivan/Programming/ML/Summarization/Samsung/gpt_summarizer_dataset.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(gpt_summarizer_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_summarizer_dataset import GPTHeadlineDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gpt_training_dataset.pkl', 'rb') as f:\n",
    "#     train_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('gpt_val_dataset.pkl', 'rb') as f:\n",
    "#     val_dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GPTHeadlineDataset(\n",
    "    tokenizer,\n",
    "    summaries=[r['summary'] for r in records['train']],\n",
    "    contents=[r['text'] for r in records['train']],\n",
    "    max_input_length=601,\n",
    "    max_summary_length=163\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = GPTHeadlineDataset(\n",
    "    tokenizer,\n",
    "    summaries=[r['summary'] for r in records['val']],\n",
    "    contents=[r['text'] for r in records['val']],\n",
    "    max_input_length=601,\n",
    "    max_summary_length=163\n",
    ")\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хак для быстрой загрузки датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('gpt_training_dataset.pkl', 'wb') as of:\n",
    "    pickle.dump(train_dataset, of)\n",
    "    \n",
    "with open('gpt_val_dataset.pkl', 'wb') as of:\n",
    "    pickle.dump(val_dataset, of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52400.000000\n",
       "mean        63.571126\n",
       "std         16.982161\n",
       "min         17.000000\n",
       "25%         51.000000\n",
       "50%         63.000000\n",
       "75%         75.000000\n",
       "max        123.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataset.summary_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52400.000000\n",
       "mean       954.682214\n",
       "std        273.714437\n",
       "min         48.000000\n",
       "25%        780.000000\n",
       "50%        901.000000\n",
       "75%       1088.000000\n",
       "max       2244.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_dataset.content_lengths).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = dict(learning_rate=6e-5, \n",
    "                    warmup_steps=1000,\n",
    "                    linear_decay_steps=0,\n",
    "                    content_loss_weight=1,\n",
    "                    summary_loss_weight=1\n",
    "                    pretrained_model_path=model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GPTSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'GPTSummarizer' from '/home/ivan/Programming/ML/Summarization/Samsung/GPTSummarizer.py'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(GPTSummarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GPTSummarizer import GPTSummarizerPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTSummarizerPL(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_device(model):\n",
    "    return next(iter(model.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_device(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_headline(model, text, max_input_length, max_output_length, **generate_args):\n",
    "    vocab=tokenizer.get_vocab()\n",
    "    bos_token_id = vocab['<s>']\n",
    "    eos_token_id = vocab['</s>']\n",
    "    pad_token_id = vocab['<pad>']\n",
    "    encoded_text = [bos_token_id] +\\\n",
    "        tokenizer.encode(text)[:max_input_length] + [eos_token_id]\n",
    "    encoded_text = torch.tensor(encoded_text, device=get_model_device(model)).view(1,-1)\n",
    "    print(encoded_text.shape)\n",
    "    encoded_output = model.gpt.generate(encoded_text,\n",
    "                                        bos_token_id=bos_token_id,\n",
    "                                        eos_token_ids=[eos_token_id],\n",
    "                                        pad_token_id=pad_token_id,\n",
    "                                        max_length=max_input_length + max_output_length + 2,\n",
    "                                        **generate_args)\n",
    "    result = tokenizer.decode(encoded_output[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2\n",
    "n_workers=1\n",
    "data_loaders = {\n",
    "    \"train\": DataLoader(train_dataset, \n",
    "                        batch_size=batch_size, num_workers=n_workers,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=train_dataset.collate),\n",
    "    \"val\": DataLoader(val_dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        num_workers=n_workers,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=val_dataset.collate),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplingCallback(Callback):\n",
    "    def on_validation_end(self, trainer, module):\n",
    "        state = module.training\n",
    "        module.train(False)\n",
    "        \n",
    "        rand_index = random.randrange(len(records['val']))\n",
    "        text = records['val'][rand_index]['text']\n",
    "        with torch.no_grad():\n",
    "            print(generate_headline(module, text, 600, 100))\n",
    "        module.train(state)\n",
    "        \n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n",
    "        if trainer.global_step % 1000 == 0:\n",
    "            state = pl_module.training\n",
    "            pl_module.train(False)\n",
    "\n",
    "            rand_index = random.randrange(len(records['val']))\n",
    "            text = records['val'][rand_index]['text']\n",
    "            with torch.no_grad():\n",
    "                print(generate_headline(pl_module, text, 600, 100))\n",
    "            pl_module.train(state)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='avg_val_loss',\n",
    "   min_delta=0.00,\n",
    "   patience=1,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('gpt_checkpoint_gazeta3', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"gpt_checkpoint_gazeta3\",monitor='avg_val_loss', mode='min', save_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[0],max_epochs=5, accumulate_grad_batches=8,\n",
    "                     callbacks=[checkpoint, early_stop_callback, SamplingCallback()], fast_dev_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, train_dataloader=data_loaders['train'], val_dataloaders=data_loaders['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model': model.gpt.state_dict, 'hparams': model.hparams}, 'gpt_ckpt_after_epoch_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_device(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,4,5].index(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary(model, text, max_input_length, max_output_length, **generate_args):\n",
    "    vocab=tokenizer.get_vocab()\n",
    "    bos_token_id = vocab['<s>']\n",
    "    eos_token_id = vocab['</s>']\n",
    "    pad_token_id = vocab['<pad>']\n",
    "    encoded_text = [bos_token_id] +\\\n",
    "        tokenizer.encode(text)[:max_input_length] + [eos_token_id]\n",
    "    encoded_text = torch.tensor(encoded_text, device=get_model_device(model)).view(1,-1)\n",
    "    print(encoded_text.shape)\n",
    "    encoded_output = model.gpt.generate(encoded_text,\n",
    "                                        bos_token_id=bos_token_id,\n",
    "                                        eos_token_ids=[eos_token_id],\n",
    "                                        pad_token_id=pad_token_id,\n",
    "                                        max_length=max_input_length + max_output_length + 2,\n",
    "                                        **generate_args)\n",
    "    \n",
    "    indices = encoded_output[0].tolist()\n",
    "    \n",
    "    first_eos_index = indices.index(eos_token_id)\n",
    "    sum_start_index = first_eos_index + 1\n",
    "    \n",
    "    final_indices = []\n",
    "    for idx in indices[sum_start_index:]:\n",
    "        if idx != eos_token_id:\n",
    "            final_indices.append(idx)\n",
    "        else:\n",
    "            break\n",
    "    return tokenizer.decode(final_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.extract_summary(model, text, max_input_length, max_output_length, **generate_args)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3206\n",
      "В Гатчине, городе Ленинградской области, крещение годовалого ребенка в Мариенбургской церкви закончилось скандалом после размещенного в сети видео. На кадрах видно, как священник пытается насильно окунуть ребенка в купель, в которую тот не помещается. Малыш кричит, плачет и вырывается из рук батюшки, на что служитель церкви не обращает внимания. «Он делал все с болью для ребенка, видел, что он взрослый, что его в такую маленькую купель не погрузить, поливать с головы надо. Но решил делать по-своему. Малыш кричал, вырывался. Я испугалась, подбежала, начала забирать. Сама чуть не загорелась, так как платком потушила свечи у купели», — рассказала «Фонтанке» мать ребенка Анастасия. Она отметила, что вместо того, чтобы отдать ей ребенка, батюшка посоветовал ей не лезть не в свое дело. «У меня были случаи, когда дети бились головой и справляли нужду в руках», — поделился опытом священник. В результате крещения малыш, по словам его матери, получил травмы. «У ребенка царапины на шее и на плече, он теперь всего панически боится, у него истерика», — продолжила Анастасия. Она отметила, что индивидуальное таинство крещения обошлось семье в 3500 руб. В то же время Анастасия обратила внимание, что процесс не заладился с самого начала. «Перед таинством на пороге церкви нас встретил батюшка Фотий и спросил у меня: «Отче наш наизусть знаешь? Нет? Причащалась? Тоже нет? Кто ты тогда для ребенка? Зачем его крестить принесла?» — вспоминает девушка. Она добавила, что тот же диалог состоялся и с другими родственниками. О проведенном ритуале родители малыша неоднократно пожалели и решили не оставлять это дело. Анастасия написала заявление в полицию и обратилась в епархиальное управление. Однако от медэкспертизы девушка отказалась, опасаясь за психику ребенка. По словам матери, ей позвонили в воскресенье, 11 августа, из епархии и извинились за инцидент. «Я спросила у позвонившего: «Вы как считаете, на видео — это насилие или это нормальный обряд крещения?» и мне сказали: «Насилие — это мягко сказано», — рассказала Анастасия. Между тем в пресс-службе Гатчинской епархии Русской православной церкви ( РПЦ ) заявили, что епископ Митрофан осведомлен о произошедшем. В связи с тем, что поведение игумена Фотия «не соответствует образу священнослужителя», епископ отстранил его от совершения таинства, передает РИА «Новости». Как отметили в пресс-службе, это одно из самых суровых дисциплинарных наказаний в церкви, не считая лишения сана. Тем временем сам Фотий не считает себя виновным. Он уверен в том, что все сделал правильно, так как выполнял «свой священнический долг», передает «Фонтанка». «А что собственно случилось-то? Ничего же не случилось, моей вины никакой нет», — заявил священнослужитель. Он утверждает, что вся проблема в «повышенных эмоциях мамы», которая «оказалась человеком невоцерковленным и не готовой к крещению». «По правилам православной церкви положено младенца погружать трижды с головой, я это и сделал. Воды младенец не наглотался, о стенки купели не ударился. Я 26 лет служу и всегда стараюсь так крестить», — отметил игумен. По его словам, ребенка было необходимо именно окунать в купель, так как именно «погружательное» крещение считается настоящим. В то время как поливают голову некрещенного человека только в случае смертельной опасности. В то же время в РПЦ уверены, что поведение игумена недопустимо и должно быть расследовано епархией, заявил ТАСС руководитель пресс-службы патриарха Московского и всея Руси священник Александр Волков. «С этим должна будет разобраться епархия. Несомненно, вопрос требует самого тщательного расследования, такие вещи совершенно не допустимы», — сказал собеседник агентства. Подобное мнение выразил и заместитель управляющего делами Московской патриархии епископ Зеленоградский Савва, написавший в своем телеграм-канале, что игумен Фотий проявил себя грубо по отношению к ребенку. «По каналам распространилась информация с приложением съемки о священнике, который, судя по видео, совершил таинство крещения грубо, а с людьми общался, судя по описанию, неадекватно Поведение священника, которое отражено на видео, конечно же недопустимо. Такого быть не должно», — указал он.\n",
      "-----------\n",
      "В Ленинградской области игумен Мариенбургской церкви провел таинство крещения годовалого ребенка, которое закончилось скандалом. В сеть попало видео, на котором видно, как ребенок вырывается из рук священника, кричит и плачет. Однако батюшка на это не обращает внимания. Мать ребенка обратилась в полицию — по ее словам, после ритуала малыш стал панически всего бояться. В Гатчинской епархии заявили, что игумен отстранен от совершения таинства.\n",
      "torch.Size([1, 602])\n",
      "В Ленинградской области крестили годовалого ребенка в Мариенбургской церкви. После того, как священник окунул его в купель, ребенок кричит, плачет и вырывается из рук батюшки. В итоге священник решил не брать ребенка с собой на обряд, а окунуть его в Мариенбургскую купель. В епархии заявили, что крещение не было «по-настоящему».\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    rand_index = random.randrange(len(records['val']))\n",
    "    print(rand_index)\n",
    "    text = records['val'][rand_index]['text']\n",
    "    print(text)\n",
    "    print(\"-----------\")\n",
    "    ref = records['val'][rand_index]['summary']\n",
    "    print(ref)\n",
    "    print(extract_summary(model,text, 600,128))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
